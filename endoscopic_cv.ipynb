{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEVslgReOXZe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Concatenate, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/dataset\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "val_dir = os.path.join(base_dir, \"val\")\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "\n",
        "color_ranges = {\n",
        "    'ulcer': (np.array([0, 0, 200]), np.array([50, 50, 255])),\n",
        "    'bleeding': (np.array([0, 50, 50]), np.array([10, 255, 255])),\n",
        "    'erythema': (np.array([160, 50, 50]), np.array([180, 255, 255])),\n",
        "    'foreign body': (np.array([25, 52, 72]), np.array([102, 255, 255])),\n",
        "    'lymphangiectasia': (np.array([20, 40, 150]), np.array([80, 255, 255])),\n",
        "    'polyp': (np.array([0, 100, 100]), np.array([10, 255, 255])),\n",
        "    'angioectasia': (np.array([160, 100, 50]), np.array([180, 255, 255])),\n",
        "    'erosion': (np.array([0, 50, 100]), np.array([50, 255, 255])),\n",
        "    'worms': (np.array([50, 100, 100]), np.array([80, 255, 255])),\n",
        "    'normal': (np.array([0, 0, 0]), np.array([180, 255, 255]))\n",
        "}\n",
        "\n",
        "def color_detection(img, color_range):\n",
        "    hsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
        "    mask = cv2.inRange(hsv_img, color_range[0], color_range[1])\n",
        "    result = cv2.bitwise_and(img, img, mask=mask)\n",
        "    return result\n",
        "\n",
        "def contour_detection(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    gray = (gray * 255).astype(np.uint8)\n",
        "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    return len(contours)\n",
        "\n",
        "def texture_analysis(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    gray = (gray * 255).astype(np.uint8)\n",
        "    glcm = graycomatrix(gray, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
        "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
        "    dissimilarity = graycoprops(glcm, 'dissimilarity')[0, 0]\n",
        "    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
        "    return np.array([contrast, dissimilarity, homogeneity])\n",
        "\n",
        "def preprocess_image(img_path, label):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, image_size)\n",
        "    img = img / 255.0\n",
        "    img_np = img.numpy()\n",
        "\n",
        "    ulcer_result = color_detection(img_np, color_ranges['ulcer'])\n",
        "    bleeding_result = color_detection(img_np, color_ranges['bleeding'])\n",
        "    erythema_result = color_detection(img_np, color_ranges['erythema'])\n",
        "\n",
        "    contours = contour_detection(img_np)\n",
        "    texture = texture_analysis(img_np)\n",
        "\n",
        "    extra_features = np.concatenate([\n",
        "        np.mean(ulcer_result, axis=(0, 1)),\n",
        "        np.mean(bleeding_result, axis=(0, 1)),\n",
        "        np.mean(erythema_result, axis=(0, 1)),\n",
        "        [contours],\n",
        "        texture\n",
        "    ])\n",
        "\n",
        "    return img.numpy(), extra_features, label\n",
        "\n",
        "def ensure_shape(image, extra_features, label):\n",
        "    image = tf.ensure_shape(image, (224, 224, 3))\n",
        "    extra_features = tf.ensure_shape(extra_features, (13,))\n",
        "    label = tf.ensure_shape(label, ())\n",
        "    return (image, extra_features), label\n",
        "\n",
        "def create_dataset(directory):\n",
        "    class_names = sorted(os.listdir(directory))\n",
        "    class_dict = {name: i for i, name in enumerate(class_names)}\n",
        "\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(directory, class_name)\n",
        "        for img_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            image_paths.append(img_path)\n",
        "            labels.append(class_dict[class_name])\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "    dataset = dataset.map(lambda x, y: tf.py_function(preprocess_image, [x, y], [tf.float32, tf.float32, tf.int32]),\n",
        "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(ensure_shape)\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "train_dataset = create_dataset(train_dir)\n",
        "val_dataset = create_dataset(val_dir)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=image_size + (3,))\n",
        "\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "image_input = Input(shape=image_size + (3,))\n",
        "extra_input = Input(shape=(13,))\n",
        "\n",
        "x = base_model(image_input)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "concatenated = Concatenate()([x, extra_input])\n",
        "\n",
        "x = Dense(512, activation='relu')(concatenated)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=[image_input, extra_input], outputs=predictions)\n",
        "\n",
        "optimizer = Adam(learning_rate=1e-4)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[lr_scheduler, early_stopping]\n",
        ")\n",
        "\n",
        "model.save(os.path.join(base_dir, 'advanced_ulcer_classification_model.h5'))\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}